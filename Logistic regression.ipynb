{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f79c92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71a923c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9cdf15ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n  Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n  Mathematical Statistics\" (John Wiley, NY, 1950).\\n- Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n  (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n- Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n  Structure and Classification Rule for Recognition in Partially Exposed\\n  Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n  Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n- Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n  on Information Theory, May 1972, 431-433.\\n- See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n  conceptual clustering system finds 3 classes in the data.\\n- Many, many more ...\\n\\n|details-end|',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'iris.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3cf5471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[5.1, 3.5, 1.4, 0.2],\n",
       "       [4.9, 3. , 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.3, 0.2],\n",
       "       [4.6, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.6, 1.4, 0.2],\n",
       "       [5.4, 3.9, 1.7, 0.4],\n",
       "       [4.6, 3.4, 1.4, 0.3],\n",
       "       [5. , 3.4, 1.5, 0.2],\n",
       "       [4.4, 2.9, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.1],\n",
       "       [5.4, 3.7, 1.5, 0.2],\n",
       "       [4.8, 3.4, 1.6, 0.2],\n",
       "       [4.8, 3. , 1.4, 0.1],\n",
       "       [4.3, 3. , 1.1, 0.1],\n",
       "       [5.8, 4. , 1.2, 0.2],\n",
       "       [5.7, 4.4, 1.5, 0.4],\n",
       "       [5.4, 3.9, 1.3, 0.4],\n",
       "       [5.1, 3.5, 1.4, 0.3],\n",
       "       [5.7, 3.8, 1.7, 0.3],\n",
       "       [5.1, 3.8, 1.5, 0.3],\n",
       "       [5.4, 3.4, 1.7, 0.2],\n",
       "       [5.1, 3.7, 1.5, 0.4],\n",
       "       [4.6, 3.6, 1. , 0.2],\n",
       "       [5.1, 3.3, 1.7, 0.5],\n",
       "       [4.8, 3.4, 1.9, 0.2],\n",
       "       [5. , 3. , 1.6, 0.2],\n",
       "       [5. , 3.4, 1.6, 0.4],\n",
       "       [5.2, 3.5, 1.5, 0.2],\n",
       "       [5.2, 3.4, 1.4, 0.2],\n",
       "       [4.7, 3.2, 1.6, 0.2],\n",
       "       [4.8, 3.1, 1.6, 0.2],\n",
       "       [5.4, 3.4, 1.5, 0.4],\n",
       "       [5.2, 4.1, 1.5, 0.1],\n",
       "       [5.5, 4.2, 1.4, 0.2],\n",
       "       [4.9, 3.1, 1.5, 0.2],\n",
       "       [5. , 3.2, 1.2, 0.2],\n",
       "       [5.5, 3.5, 1.3, 0.2],\n",
       "       [4.9, 3.6, 1.4, 0.1],\n",
       "       [4.4, 3. , 1.3, 0.2],\n",
       "       [5.1, 3.4, 1.5, 0.2],\n",
       "       [5. , 3.5, 1.3, 0.3],\n",
       "       [4.5, 2.3, 1.3, 0.3],\n",
       "       [4.4, 3.2, 1.3, 0.2],\n",
       "       [5. , 3.5, 1.6, 0.6],\n",
       "       [5.1, 3.8, 1.9, 0.4],\n",
       "       [4.8, 3. , 1.4, 0.3],\n",
       "       [5.1, 3.8, 1.6, 0.2],\n",
       "       [4.6, 3.2, 1.4, 0.2],\n",
       "       [5.3, 3.7, 1.5, 0.2],\n",
       "       [5. , 3.3, 1.4, 0.2],\n",
       "       [7. , 3.2, 4.7, 1.4],\n",
       "       [6.4, 3.2, 4.5, 1.5],\n",
       "       [6.9, 3.1, 4.9, 1.5],\n",
       "       [5.5, 2.3, 4. , 1.3],\n",
       "       [6.5, 2.8, 4.6, 1.5],\n",
       "       [5.7, 2.8, 4.5, 1.3],\n",
       "       [6.3, 3.3, 4.7, 1.6],\n",
       "       [4.9, 2.4, 3.3, 1. ],\n",
       "       [6.6, 2.9, 4.6, 1.3],\n",
       "       [5.2, 2.7, 3.9, 1.4],\n",
       "       [5. , 2. , 3.5, 1. ],\n",
       "       [5.9, 3. , 4.2, 1.5],\n",
       "       [6. , 2.2, 4. , 1. ],\n",
       "       [6.1, 2.9, 4.7, 1.4],\n",
       "       [5.6, 2.9, 3.6, 1.3],\n",
       "       [6.7, 3.1, 4.4, 1.4],\n",
       "       [5.6, 3. , 4.5, 1.5],\n",
       "       [5.8, 2.7, 4.1, 1. ],\n",
       "       [6.2, 2.2, 4.5, 1.5],\n",
       "       [5.6, 2.5, 3.9, 1.1],\n",
       "       [5.9, 3.2, 4.8, 1.8],\n",
       "       [6.1, 2.8, 4. , 1.3],\n",
       "       [6.3, 2.5, 4.9, 1.5],\n",
       "       [6.1, 2.8, 4.7, 1.2],\n",
       "       [6.4, 2.9, 4.3, 1.3],\n",
       "       [6.6, 3. , 4.4, 1.4],\n",
       "       [6.8, 2.8, 4.8, 1.4],\n",
       "       [6.7, 3. , 5. , 1.7],\n",
       "       [6. , 2.9, 4.5, 1.5],\n",
       "       [5.7, 2.6, 3.5, 1. ],\n",
       "       [5.5, 2.4, 3.8, 1.1],\n",
       "       [5.5, 2.4, 3.7, 1. ],\n",
       "       [5.8, 2.7, 3.9, 1.2],\n",
       "       [6. , 2.7, 5.1, 1.6],\n",
       "       [5.4, 3. , 4.5, 1.5],\n",
       "       [6. , 3.4, 4.5, 1.6],\n",
       "       [6.7, 3.1, 4.7, 1.5],\n",
       "       [6.3, 2.3, 4.4, 1.3],\n",
       "       [5.6, 3. , 4.1, 1.3],\n",
       "       [5.5, 2.5, 4. , 1.3],\n",
       "       [5.5, 2.6, 4.4, 1.2],\n",
       "       [6.1, 3. , 4.6, 1.4],\n",
       "       [5.8, 2.6, 4. , 1.2],\n",
       "       [5. , 2.3, 3.3, 1. ],\n",
       "       [5.6, 2.7, 4.2, 1.3],\n",
       "       [5.7, 3. , 4.2, 1.2],\n",
       "       [5.7, 2.9, 4.2, 1.3],\n",
       "       [6.2, 2.9, 4.3, 1.3],\n",
       "       [5.1, 2.5, 3. , 1.1],\n",
       "       [5.7, 2.8, 4.1, 1.3],\n",
       "       [6.3, 3.3, 6. , 2.5],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [7.1, 3. , 5.9, 2.1],\n",
       "       [6.3, 2.9, 5.6, 1.8],\n",
       "       [6.5, 3. , 5.8, 2.2],\n",
       "       [7.6, 3. , 6.6, 2.1],\n",
       "       [4.9, 2.5, 4.5, 1.7],\n",
       "       [7.3, 2.9, 6.3, 1.8],\n",
       "       [6.7, 2.5, 5.8, 1.8],\n",
       "       [7.2, 3.6, 6.1, 2.5],\n",
       "       [6.5, 3.2, 5.1, 2. ],\n",
       "       [6.4, 2.7, 5.3, 1.9],\n",
       "       [6.8, 3. , 5.5, 2.1],\n",
       "       [5.7, 2.5, 5. , 2. ],\n",
       "       [5.8, 2.8, 5.1, 2.4],\n",
       "       [6.4, 3.2, 5.3, 2.3],\n",
       "       [6.5, 3. , 5.5, 1.8],\n",
       "       [7.7, 3.8, 6.7, 2.2],\n",
       "       [7.7, 2.6, 6.9, 2.3],\n",
       "       [6. , 2.2, 5. , 1.5],\n",
       "       [6.9, 3.2, 5.7, 2.3],\n",
       "       [5.6, 2.8, 4.9, 2. ],\n",
       "       [7.7, 2.8, 6.7, 2. ],\n",
       "       [6.3, 2.7, 4.9, 1.8],\n",
       "       [6.7, 3.3, 5.7, 2.1],\n",
       "       [7.2, 3.2, 6. , 1.8],\n",
       "       [6.2, 2.8, 4.8, 1.8],\n",
       "       [6.1, 3. , 4.9, 1.8],\n",
       "       [6.4, 2.8, 5.6, 2.1],\n",
       "       [7.2, 3. , 5.8, 1.6],\n",
       "       [7.4, 2.8, 6.1, 1.9],\n",
       "       [7.9, 3.8, 6.4, 2. ],\n",
       "       [6.4, 2.8, 5.6, 2.2],\n",
       "       [6.3, 2.8, 5.1, 1.5],\n",
       "       [6.1, 2.6, 5.6, 1.4],\n",
       "       [7.7, 3. , 6.1, 2.3],\n",
       "       [6.3, 3.4, 5.6, 2.4],\n",
       "       [6.4, 3.1, 5.5, 1.8],\n",
       "       [6. , 3. , 4.8, 1.8],\n",
       "       [6.9, 3.1, 5.4, 2.1],\n",
       "       [6.7, 3.1, 5.6, 2.4],\n",
       "       [6.9, 3.1, 5.1, 2.3],\n",
       "       [5.8, 2.7, 5.1, 1.9],\n",
       "       [6.8, 3.2, 5.9, 2.3],\n",
       "       [6.7, 3.3, 5.7, 2.5],\n",
       "       [6.7, 3. , 5.2, 2.3],\n",
       "       [6.3, 2.5, 5. , 1.9],\n",
       "       [6.5, 3. , 5.2, 2. ],\n",
       "       [6.2, 3.4, 5.4, 2.3],\n",
       "       [5.9, 3. , 5.1, 1.8]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "75e965b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target']   #1-setosa, 2-versicolour, 3-virginica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "755e1781",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=iris['data'][:,3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8b749258",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [0.3],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.1],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.1],\n",
       "       [0.1],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [0.4],\n",
       "       [0.3],\n",
       "       [0.3],\n",
       "       [0.3],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [0.2],\n",
       "       [0.5],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.4],\n",
       "       [0.1],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.1],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.3],\n",
       "       [0.3],\n",
       "       [0.2],\n",
       "       [0.6],\n",
       "       [0.4],\n",
       "       [0.3],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [0.2],\n",
       "       [1.4],\n",
       "       [1.5],\n",
       "       [1.5],\n",
       "       [1.3],\n",
       "       [1.5],\n",
       "       [1.3],\n",
       "       [1.6],\n",
       "       [1. ],\n",
       "       [1.3],\n",
       "       [1.4],\n",
       "       [1. ],\n",
       "       [1.5],\n",
       "       [1. ],\n",
       "       [1.4],\n",
       "       [1.3],\n",
       "       [1.4],\n",
       "       [1.5],\n",
       "       [1. ],\n",
       "       [1.5],\n",
       "       [1.1],\n",
       "       [1.8],\n",
       "       [1.3],\n",
       "       [1.5],\n",
       "       [1.2],\n",
       "       [1.3],\n",
       "       [1.4],\n",
       "       [1.4],\n",
       "       [1.7],\n",
       "       [1.5],\n",
       "       [1. ],\n",
       "       [1.1],\n",
       "       [1. ],\n",
       "       [1.2],\n",
       "       [1.6],\n",
       "       [1.5],\n",
       "       [1.6],\n",
       "       [1.5],\n",
       "       [1.3],\n",
       "       [1.3],\n",
       "       [1.3],\n",
       "       [1.2],\n",
       "       [1.4],\n",
       "       [1.2],\n",
       "       [1. ],\n",
       "       [1.3],\n",
       "       [1.2],\n",
       "       [1.3],\n",
       "       [1.3],\n",
       "       [1.1],\n",
       "       [1.3],\n",
       "       [2.5],\n",
       "       [1.9],\n",
       "       [2.1],\n",
       "       [1.8],\n",
       "       [2.2],\n",
       "       [2.1],\n",
       "       [1.7],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [2.5],\n",
       "       [2. ],\n",
       "       [1.9],\n",
       "       [2.1],\n",
       "       [2. ],\n",
       "       [2.4],\n",
       "       [2.3],\n",
       "       [1.8],\n",
       "       [2.2],\n",
       "       [2.3],\n",
       "       [1.5],\n",
       "       [2.3],\n",
       "       [2. ],\n",
       "       [2. ],\n",
       "       [1.8],\n",
       "       [2.1],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [2.1],\n",
       "       [1.6],\n",
       "       [1.9],\n",
       "       [2. ],\n",
       "       [2.2],\n",
       "       [1.5],\n",
       "       [1.4],\n",
       "       [2.3],\n",
       "       [2.4],\n",
       "       [1.8],\n",
       "       [1.8],\n",
       "       [2.1],\n",
       "       [2.4],\n",
       "       [2.3],\n",
       "       [1.9],\n",
       "       [2.3],\n",
       "       [2.5],\n",
       "       [2.3],\n",
       "       [1.9],\n",
       "       [2. ],\n",
       "       [2.3],\n",
       "       [1.8]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02cea2fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iris['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ac3e1ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "y= (iris['target'] == 2)  #if target==2 then it will return (true)1, else (false)0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "900d3303",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a54d32b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d77a72d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = clf.predict(([[2.6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0873469b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True]\n"
     ]
    }
   ],
   "source": [
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c613fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = clf.predict(([[1.6]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1baf6b7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[False]\n"
     ]
    }
   ],
   "source": [
    "print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8215a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#using matplotlib to plot the visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e473d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new=np.linspace(0,4,1000).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "01e65c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        ],\n",
       "       [0.004004  ],\n",
       "       [0.00800801],\n",
       "       [0.01201201],\n",
       "       [0.01601602],\n",
       "       [0.02002002],\n",
       "       [0.02402402],\n",
       "       [0.02802803],\n",
       "       [0.03203203],\n",
       "       [0.03603604],\n",
       "       [0.04004004],\n",
       "       [0.04404404],\n",
       "       [0.04804805],\n",
       "       [0.05205205],\n",
       "       [0.05605606],\n",
       "       [0.06006006],\n",
       "       [0.06406406],\n",
       "       [0.06806807],\n",
       "       [0.07207207],\n",
       "       [0.07607608],\n",
       "       [0.08008008],\n",
       "       [0.08408408],\n",
       "       [0.08808809],\n",
       "       [0.09209209],\n",
       "       [0.0960961 ],\n",
       "       [0.1001001 ],\n",
       "       [0.1041041 ],\n",
       "       [0.10810811],\n",
       "       [0.11211211],\n",
       "       [0.11611612],\n",
       "       [0.12012012],\n",
       "       [0.12412412],\n",
       "       [0.12812813],\n",
       "       [0.13213213],\n",
       "       [0.13613614],\n",
       "       [0.14014014],\n",
       "       [0.14414414],\n",
       "       [0.14814815],\n",
       "       [0.15215215],\n",
       "       [0.15615616],\n",
       "       [0.16016016],\n",
       "       [0.16416416],\n",
       "       [0.16816817],\n",
       "       [0.17217217],\n",
       "       [0.17617618],\n",
       "       [0.18018018],\n",
       "       [0.18418418],\n",
       "       [0.18818819],\n",
       "       [0.19219219],\n",
       "       [0.1961962 ],\n",
       "       [0.2002002 ],\n",
       "       [0.2042042 ],\n",
       "       [0.20820821],\n",
       "       [0.21221221],\n",
       "       [0.21621622],\n",
       "       [0.22022022],\n",
       "       [0.22422422],\n",
       "       [0.22822823],\n",
       "       [0.23223223],\n",
       "       [0.23623624],\n",
       "       [0.24024024],\n",
       "       [0.24424424],\n",
       "       [0.24824825],\n",
       "       [0.25225225],\n",
       "       [0.25625626],\n",
       "       [0.26026026],\n",
       "       [0.26426426],\n",
       "       [0.26826827],\n",
       "       [0.27227227],\n",
       "       [0.27627628],\n",
       "       [0.28028028],\n",
       "       [0.28428428],\n",
       "       [0.28828829],\n",
       "       [0.29229229],\n",
       "       [0.2962963 ],\n",
       "       [0.3003003 ],\n",
       "       [0.3043043 ],\n",
       "       [0.30830831],\n",
       "       [0.31231231],\n",
       "       [0.31631632],\n",
       "       [0.32032032],\n",
       "       [0.32432432],\n",
       "       [0.32832833],\n",
       "       [0.33233233],\n",
       "       [0.33633634],\n",
       "       [0.34034034],\n",
       "       [0.34434434],\n",
       "       [0.34834835],\n",
       "       [0.35235235],\n",
       "       [0.35635636],\n",
       "       [0.36036036],\n",
       "       [0.36436436],\n",
       "       [0.36836837],\n",
       "       [0.37237237],\n",
       "       [0.37637638],\n",
       "       [0.38038038],\n",
       "       [0.38438438],\n",
       "       [0.38838839],\n",
       "       [0.39239239],\n",
       "       [0.3963964 ],\n",
       "       [0.4004004 ],\n",
       "       [0.4044044 ],\n",
       "       [0.40840841],\n",
       "       [0.41241241],\n",
       "       [0.41641642],\n",
       "       [0.42042042],\n",
       "       [0.42442442],\n",
       "       [0.42842843],\n",
       "       [0.43243243],\n",
       "       [0.43643644],\n",
       "       [0.44044044],\n",
       "       [0.44444444],\n",
       "       [0.44844845],\n",
       "       [0.45245245],\n",
       "       [0.45645646],\n",
       "       [0.46046046],\n",
       "       [0.46446446],\n",
       "       [0.46846847],\n",
       "       [0.47247247],\n",
       "       [0.47647648],\n",
       "       [0.48048048],\n",
       "       [0.48448448],\n",
       "       [0.48848849],\n",
       "       [0.49249249],\n",
       "       [0.4964965 ],\n",
       "       [0.5005005 ],\n",
       "       [0.5045045 ],\n",
       "       [0.50850851],\n",
       "       [0.51251251],\n",
       "       [0.51651652],\n",
       "       [0.52052052],\n",
       "       [0.52452452],\n",
       "       [0.52852853],\n",
       "       [0.53253253],\n",
       "       [0.53653654],\n",
       "       [0.54054054],\n",
       "       [0.54454454],\n",
       "       [0.54854855],\n",
       "       [0.55255255],\n",
       "       [0.55655656],\n",
       "       [0.56056056],\n",
       "       [0.56456456],\n",
       "       [0.56856857],\n",
       "       [0.57257257],\n",
       "       [0.57657658],\n",
       "       [0.58058058],\n",
       "       [0.58458458],\n",
       "       [0.58858859],\n",
       "       [0.59259259],\n",
       "       [0.5965966 ],\n",
       "       [0.6006006 ],\n",
       "       [0.6046046 ],\n",
       "       [0.60860861],\n",
       "       [0.61261261],\n",
       "       [0.61661662],\n",
       "       [0.62062062],\n",
       "       [0.62462462],\n",
       "       [0.62862863],\n",
       "       [0.63263263],\n",
       "       [0.63663664],\n",
       "       [0.64064064],\n",
       "       [0.64464464],\n",
       "       [0.64864865],\n",
       "       [0.65265265],\n",
       "       [0.65665666],\n",
       "       [0.66066066],\n",
       "       [0.66466466],\n",
       "       [0.66866867],\n",
       "       [0.67267267],\n",
       "       [0.67667668],\n",
       "       [0.68068068],\n",
       "       [0.68468468],\n",
       "       [0.68868869],\n",
       "       [0.69269269],\n",
       "       [0.6966967 ],\n",
       "       [0.7007007 ],\n",
       "       [0.7047047 ],\n",
       "       [0.70870871],\n",
       "       [0.71271271],\n",
       "       [0.71671672],\n",
       "       [0.72072072],\n",
       "       [0.72472472],\n",
       "       [0.72872873],\n",
       "       [0.73273273],\n",
       "       [0.73673674],\n",
       "       [0.74074074],\n",
       "       [0.74474474],\n",
       "       [0.74874875],\n",
       "       [0.75275275],\n",
       "       [0.75675676],\n",
       "       [0.76076076],\n",
       "       [0.76476476],\n",
       "       [0.76876877],\n",
       "       [0.77277277],\n",
       "       [0.77677678],\n",
       "       [0.78078078],\n",
       "       [0.78478478],\n",
       "       [0.78878879],\n",
       "       [0.79279279],\n",
       "       [0.7967968 ],\n",
       "       [0.8008008 ],\n",
       "       [0.8048048 ],\n",
       "       [0.80880881],\n",
       "       [0.81281281],\n",
       "       [0.81681682],\n",
       "       [0.82082082],\n",
       "       [0.82482482],\n",
       "       [0.82882883],\n",
       "       [0.83283283],\n",
       "       [0.83683684],\n",
       "       [0.84084084],\n",
       "       [0.84484484],\n",
       "       [0.84884885],\n",
       "       [0.85285285],\n",
       "       [0.85685686],\n",
       "       [0.86086086],\n",
       "       [0.86486486],\n",
       "       [0.86886887],\n",
       "       [0.87287287],\n",
       "       [0.87687688],\n",
       "       [0.88088088],\n",
       "       [0.88488488],\n",
       "       [0.88888889],\n",
       "       [0.89289289],\n",
       "       [0.8968969 ],\n",
       "       [0.9009009 ],\n",
       "       [0.9049049 ],\n",
       "       [0.90890891],\n",
       "       [0.91291291],\n",
       "       [0.91691692],\n",
       "       [0.92092092],\n",
       "       [0.92492492],\n",
       "       [0.92892893],\n",
       "       [0.93293293],\n",
       "       [0.93693694],\n",
       "       [0.94094094],\n",
       "       [0.94494494],\n",
       "       [0.94894895],\n",
       "       [0.95295295],\n",
       "       [0.95695696],\n",
       "       [0.96096096],\n",
       "       [0.96496496],\n",
       "       [0.96896897],\n",
       "       [0.97297297],\n",
       "       [0.97697698],\n",
       "       [0.98098098],\n",
       "       [0.98498498],\n",
       "       [0.98898899],\n",
       "       [0.99299299],\n",
       "       [0.996997  ],\n",
       "       [1.001001  ],\n",
       "       [1.00500501],\n",
       "       [1.00900901],\n",
       "       [1.01301301],\n",
       "       [1.01701702],\n",
       "       [1.02102102],\n",
       "       [1.02502503],\n",
       "       [1.02902903],\n",
       "       [1.03303303],\n",
       "       [1.03703704],\n",
       "       [1.04104104],\n",
       "       [1.04504505],\n",
       "       [1.04904905],\n",
       "       [1.05305305],\n",
       "       [1.05705706],\n",
       "       [1.06106106],\n",
       "       [1.06506507],\n",
       "       [1.06906907],\n",
       "       [1.07307307],\n",
       "       [1.07707708],\n",
       "       [1.08108108],\n",
       "       [1.08508509],\n",
       "       [1.08908909],\n",
       "       [1.09309309],\n",
       "       [1.0970971 ],\n",
       "       [1.1011011 ],\n",
       "       [1.10510511],\n",
       "       [1.10910911],\n",
       "       [1.11311311],\n",
       "       [1.11711712],\n",
       "       [1.12112112],\n",
       "       [1.12512513],\n",
       "       [1.12912913],\n",
       "       [1.13313313],\n",
       "       [1.13713714],\n",
       "       [1.14114114],\n",
       "       [1.14514515],\n",
       "       [1.14914915],\n",
       "       [1.15315315],\n",
       "       [1.15715716],\n",
       "       [1.16116116],\n",
       "       [1.16516517],\n",
       "       [1.16916917],\n",
       "       [1.17317317],\n",
       "       [1.17717718],\n",
       "       [1.18118118],\n",
       "       [1.18518519],\n",
       "       [1.18918919],\n",
       "       [1.19319319],\n",
       "       [1.1971972 ],\n",
       "       [1.2012012 ],\n",
       "       [1.20520521],\n",
       "       [1.20920921],\n",
       "       [1.21321321],\n",
       "       [1.21721722],\n",
       "       [1.22122122],\n",
       "       [1.22522523],\n",
       "       [1.22922923],\n",
       "       [1.23323323],\n",
       "       [1.23723724],\n",
       "       [1.24124124],\n",
       "       [1.24524525],\n",
       "       [1.24924925],\n",
       "       [1.25325325],\n",
       "       [1.25725726],\n",
       "       [1.26126126],\n",
       "       [1.26526527],\n",
       "       [1.26926927],\n",
       "       [1.27327327],\n",
       "       [1.27727728],\n",
       "       [1.28128128],\n",
       "       [1.28528529],\n",
       "       [1.28928929],\n",
       "       [1.29329329],\n",
       "       [1.2972973 ],\n",
       "       [1.3013013 ],\n",
       "       [1.30530531],\n",
       "       [1.30930931],\n",
       "       [1.31331331],\n",
       "       [1.31731732],\n",
       "       [1.32132132],\n",
       "       [1.32532533],\n",
       "       [1.32932933],\n",
       "       [1.33333333],\n",
       "       [1.33733734],\n",
       "       [1.34134134],\n",
       "       [1.34534535],\n",
       "       [1.34934935],\n",
       "       [1.35335335],\n",
       "       [1.35735736],\n",
       "       [1.36136136],\n",
       "       [1.36536537],\n",
       "       [1.36936937],\n",
       "       [1.37337337],\n",
       "       [1.37737738],\n",
       "       [1.38138138],\n",
       "       [1.38538539],\n",
       "       [1.38938939],\n",
       "       [1.39339339],\n",
       "       [1.3973974 ],\n",
       "       [1.4014014 ],\n",
       "       [1.40540541],\n",
       "       [1.40940941],\n",
       "       [1.41341341],\n",
       "       [1.41741742],\n",
       "       [1.42142142],\n",
       "       [1.42542543],\n",
       "       [1.42942943],\n",
       "       [1.43343343],\n",
       "       [1.43743744],\n",
       "       [1.44144144],\n",
       "       [1.44544545],\n",
       "       [1.44944945],\n",
       "       [1.45345345],\n",
       "       [1.45745746],\n",
       "       [1.46146146],\n",
       "       [1.46546547],\n",
       "       [1.46946947],\n",
       "       [1.47347347],\n",
       "       [1.47747748],\n",
       "       [1.48148148],\n",
       "       [1.48548549],\n",
       "       [1.48948949],\n",
       "       [1.49349349],\n",
       "       [1.4974975 ],\n",
       "       [1.5015015 ],\n",
       "       [1.50550551],\n",
       "       [1.50950951],\n",
       "       [1.51351351],\n",
       "       [1.51751752],\n",
       "       [1.52152152],\n",
       "       [1.52552553],\n",
       "       [1.52952953],\n",
       "       [1.53353353],\n",
       "       [1.53753754],\n",
       "       [1.54154154],\n",
       "       [1.54554555],\n",
       "       [1.54954955],\n",
       "       [1.55355355],\n",
       "       [1.55755756],\n",
       "       [1.56156156],\n",
       "       [1.56556557],\n",
       "       [1.56956957],\n",
       "       [1.57357357],\n",
       "       [1.57757758],\n",
       "       [1.58158158],\n",
       "       [1.58558559],\n",
       "       [1.58958959],\n",
       "       [1.59359359],\n",
       "       [1.5975976 ],\n",
       "       [1.6016016 ],\n",
       "       [1.60560561],\n",
       "       [1.60960961],\n",
       "       [1.61361361],\n",
       "       [1.61761762],\n",
       "       [1.62162162],\n",
       "       [1.62562563],\n",
       "       [1.62962963],\n",
       "       [1.63363363],\n",
       "       [1.63763764],\n",
       "       [1.64164164],\n",
       "       [1.64564565],\n",
       "       [1.64964965],\n",
       "       [1.65365365],\n",
       "       [1.65765766],\n",
       "       [1.66166166],\n",
       "       [1.66566567],\n",
       "       [1.66966967],\n",
       "       [1.67367367],\n",
       "       [1.67767768],\n",
       "       [1.68168168],\n",
       "       [1.68568569],\n",
       "       [1.68968969],\n",
       "       [1.69369369],\n",
       "       [1.6976977 ],\n",
       "       [1.7017017 ],\n",
       "       [1.70570571],\n",
       "       [1.70970971],\n",
       "       [1.71371371],\n",
       "       [1.71771772],\n",
       "       [1.72172172],\n",
       "       [1.72572573],\n",
       "       [1.72972973],\n",
       "       [1.73373373],\n",
       "       [1.73773774],\n",
       "       [1.74174174],\n",
       "       [1.74574575],\n",
       "       [1.74974975],\n",
       "       [1.75375375],\n",
       "       [1.75775776],\n",
       "       [1.76176176],\n",
       "       [1.76576577],\n",
       "       [1.76976977],\n",
       "       [1.77377377],\n",
       "       [1.77777778],\n",
       "       [1.78178178],\n",
       "       [1.78578579],\n",
       "       [1.78978979],\n",
       "       [1.79379379],\n",
       "       [1.7977978 ],\n",
       "       [1.8018018 ],\n",
       "       [1.80580581],\n",
       "       [1.80980981],\n",
       "       [1.81381381],\n",
       "       [1.81781782],\n",
       "       [1.82182182],\n",
       "       [1.82582583],\n",
       "       [1.82982983],\n",
       "       [1.83383383],\n",
       "       [1.83783784],\n",
       "       [1.84184184],\n",
       "       [1.84584585],\n",
       "       [1.84984985],\n",
       "       [1.85385385],\n",
       "       [1.85785786],\n",
       "       [1.86186186],\n",
       "       [1.86586587],\n",
       "       [1.86986987],\n",
       "       [1.87387387],\n",
       "       [1.87787788],\n",
       "       [1.88188188],\n",
       "       [1.88588589],\n",
       "       [1.88988989],\n",
       "       [1.89389389],\n",
       "       [1.8978979 ],\n",
       "       [1.9019019 ],\n",
       "       [1.90590591],\n",
       "       [1.90990991],\n",
       "       [1.91391391],\n",
       "       [1.91791792],\n",
       "       [1.92192192],\n",
       "       [1.92592593],\n",
       "       [1.92992993],\n",
       "       [1.93393393],\n",
       "       [1.93793794],\n",
       "       [1.94194194],\n",
       "       [1.94594595],\n",
       "       [1.94994995],\n",
       "       [1.95395395],\n",
       "       [1.95795796],\n",
       "       [1.96196196],\n",
       "       [1.96596597],\n",
       "       [1.96996997],\n",
       "       [1.97397397],\n",
       "       [1.97797798],\n",
       "       [1.98198198],\n",
       "       [1.98598599],\n",
       "       [1.98998999],\n",
       "       [1.99399399],\n",
       "       [1.997998  ],\n",
       "       [2.002002  ],\n",
       "       [2.00600601],\n",
       "       [2.01001001],\n",
       "       [2.01401401],\n",
       "       [2.01801802],\n",
       "       [2.02202202],\n",
       "       [2.02602603],\n",
       "       [2.03003003],\n",
       "       [2.03403403],\n",
       "       [2.03803804],\n",
       "       [2.04204204],\n",
       "       [2.04604605],\n",
       "       [2.05005005],\n",
       "       [2.05405405],\n",
       "       [2.05805806],\n",
       "       [2.06206206],\n",
       "       [2.06606607],\n",
       "       [2.07007007],\n",
       "       [2.07407407],\n",
       "       [2.07807808],\n",
       "       [2.08208208],\n",
       "       [2.08608609],\n",
       "       [2.09009009],\n",
       "       [2.09409409],\n",
       "       [2.0980981 ],\n",
       "       [2.1021021 ],\n",
       "       [2.10610611],\n",
       "       [2.11011011],\n",
       "       [2.11411411],\n",
       "       [2.11811812],\n",
       "       [2.12212212],\n",
       "       [2.12612613],\n",
       "       [2.13013013],\n",
       "       [2.13413413],\n",
       "       [2.13813814],\n",
       "       [2.14214214],\n",
       "       [2.14614615],\n",
       "       [2.15015015],\n",
       "       [2.15415415],\n",
       "       [2.15815816],\n",
       "       [2.16216216],\n",
       "       [2.16616617],\n",
       "       [2.17017017],\n",
       "       [2.17417417],\n",
       "       [2.17817818],\n",
       "       [2.18218218],\n",
       "       [2.18618619],\n",
       "       [2.19019019],\n",
       "       [2.19419419],\n",
       "       [2.1981982 ],\n",
       "       [2.2022022 ],\n",
       "       [2.20620621],\n",
       "       [2.21021021],\n",
       "       [2.21421421],\n",
       "       [2.21821822],\n",
       "       [2.22222222],\n",
       "       [2.22622623],\n",
       "       [2.23023023],\n",
       "       [2.23423423],\n",
       "       [2.23823824],\n",
       "       [2.24224224],\n",
       "       [2.24624625],\n",
       "       [2.25025025],\n",
       "       [2.25425425],\n",
       "       [2.25825826],\n",
       "       [2.26226226],\n",
       "       [2.26626627],\n",
       "       [2.27027027],\n",
       "       [2.27427427],\n",
       "       [2.27827828],\n",
       "       [2.28228228],\n",
       "       [2.28628629],\n",
       "       [2.29029029],\n",
       "       [2.29429429],\n",
       "       [2.2982983 ],\n",
       "       [2.3023023 ],\n",
       "       [2.30630631],\n",
       "       [2.31031031],\n",
       "       [2.31431431],\n",
       "       [2.31831832],\n",
       "       [2.32232232],\n",
       "       [2.32632633],\n",
       "       [2.33033033],\n",
       "       [2.33433433],\n",
       "       [2.33833834],\n",
       "       [2.34234234],\n",
       "       [2.34634635],\n",
       "       [2.35035035],\n",
       "       [2.35435435],\n",
       "       [2.35835836],\n",
       "       [2.36236236],\n",
       "       [2.36636637],\n",
       "       [2.37037037],\n",
       "       [2.37437437],\n",
       "       [2.37837838],\n",
       "       [2.38238238],\n",
       "       [2.38638639],\n",
       "       [2.39039039],\n",
       "       [2.39439439],\n",
       "       [2.3983984 ],\n",
       "       [2.4024024 ],\n",
       "       [2.40640641],\n",
       "       [2.41041041],\n",
       "       [2.41441441],\n",
       "       [2.41841842],\n",
       "       [2.42242242],\n",
       "       [2.42642643],\n",
       "       [2.43043043],\n",
       "       [2.43443443],\n",
       "       [2.43843844],\n",
       "       [2.44244244],\n",
       "       [2.44644645],\n",
       "       [2.45045045],\n",
       "       [2.45445445],\n",
       "       [2.45845846],\n",
       "       [2.46246246],\n",
       "       [2.46646647],\n",
       "       [2.47047047],\n",
       "       [2.47447447],\n",
       "       [2.47847848],\n",
       "       [2.48248248],\n",
       "       [2.48648649],\n",
       "       [2.49049049],\n",
       "       [2.49449449],\n",
       "       [2.4984985 ],\n",
       "       [2.5025025 ],\n",
       "       [2.50650651],\n",
       "       [2.51051051],\n",
       "       [2.51451451],\n",
       "       [2.51851852],\n",
       "       [2.52252252],\n",
       "       [2.52652653],\n",
       "       [2.53053053],\n",
       "       [2.53453453],\n",
       "       [2.53853854],\n",
       "       [2.54254254],\n",
       "       [2.54654655],\n",
       "       [2.55055055],\n",
       "       [2.55455455],\n",
       "       [2.55855856],\n",
       "       [2.56256256],\n",
       "       [2.56656657],\n",
       "       [2.57057057],\n",
       "       [2.57457457],\n",
       "       [2.57857858],\n",
       "       [2.58258258],\n",
       "       [2.58658659],\n",
       "       [2.59059059],\n",
       "       [2.59459459],\n",
       "       [2.5985986 ],\n",
       "       [2.6026026 ],\n",
       "       [2.60660661],\n",
       "       [2.61061061],\n",
       "       [2.61461461],\n",
       "       [2.61861862],\n",
       "       [2.62262262],\n",
       "       [2.62662663],\n",
       "       [2.63063063],\n",
       "       [2.63463463],\n",
       "       [2.63863864],\n",
       "       [2.64264264],\n",
       "       [2.64664665],\n",
       "       [2.65065065],\n",
       "       [2.65465465],\n",
       "       [2.65865866],\n",
       "       [2.66266266],\n",
       "       [2.66666667],\n",
       "       [2.67067067],\n",
       "       [2.67467467],\n",
       "       [2.67867868],\n",
       "       [2.68268268],\n",
       "       [2.68668669],\n",
       "       [2.69069069],\n",
       "       [2.69469469],\n",
       "       [2.6986987 ],\n",
       "       [2.7027027 ],\n",
       "       [2.70670671],\n",
       "       [2.71071071],\n",
       "       [2.71471471],\n",
       "       [2.71871872],\n",
       "       [2.72272272],\n",
       "       [2.72672673],\n",
       "       [2.73073073],\n",
       "       [2.73473473],\n",
       "       [2.73873874],\n",
       "       [2.74274274],\n",
       "       [2.74674675],\n",
       "       [2.75075075],\n",
       "       [2.75475475],\n",
       "       [2.75875876],\n",
       "       [2.76276276],\n",
       "       [2.76676677],\n",
       "       [2.77077077],\n",
       "       [2.77477477],\n",
       "       [2.77877878],\n",
       "       [2.78278278],\n",
       "       [2.78678679],\n",
       "       [2.79079079],\n",
       "       [2.79479479],\n",
       "       [2.7987988 ],\n",
       "       [2.8028028 ],\n",
       "       [2.80680681],\n",
       "       [2.81081081],\n",
       "       [2.81481481],\n",
       "       [2.81881882],\n",
       "       [2.82282282],\n",
       "       [2.82682683],\n",
       "       [2.83083083],\n",
       "       [2.83483483],\n",
       "       [2.83883884],\n",
       "       [2.84284284],\n",
       "       [2.84684685],\n",
       "       [2.85085085],\n",
       "       [2.85485485],\n",
       "       [2.85885886],\n",
       "       [2.86286286],\n",
       "       [2.86686687],\n",
       "       [2.87087087],\n",
       "       [2.87487487],\n",
       "       [2.87887888],\n",
       "       [2.88288288],\n",
       "       [2.88688689],\n",
       "       [2.89089089],\n",
       "       [2.89489489],\n",
       "       [2.8988989 ],\n",
       "       [2.9029029 ],\n",
       "       [2.90690691],\n",
       "       [2.91091091],\n",
       "       [2.91491491],\n",
       "       [2.91891892],\n",
       "       [2.92292292],\n",
       "       [2.92692693],\n",
       "       [2.93093093],\n",
       "       [2.93493493],\n",
       "       [2.93893894],\n",
       "       [2.94294294],\n",
       "       [2.94694695],\n",
       "       [2.95095095],\n",
       "       [2.95495495],\n",
       "       [2.95895896],\n",
       "       [2.96296296],\n",
       "       [2.96696697],\n",
       "       [2.97097097],\n",
       "       [2.97497497],\n",
       "       [2.97897898],\n",
       "       [2.98298298],\n",
       "       [2.98698699],\n",
       "       [2.99099099],\n",
       "       [2.99499499],\n",
       "       [2.998999  ],\n",
       "       [3.003003  ],\n",
       "       [3.00700701],\n",
       "       [3.01101101],\n",
       "       [3.01501502],\n",
       "       [3.01901902],\n",
       "       [3.02302302],\n",
       "       [3.02702703],\n",
       "       [3.03103103],\n",
       "       [3.03503504],\n",
       "       [3.03903904],\n",
       "       [3.04304304],\n",
       "       [3.04704705],\n",
       "       [3.05105105],\n",
       "       [3.05505506],\n",
       "       [3.05905906],\n",
       "       [3.06306306],\n",
       "       [3.06706707],\n",
       "       [3.07107107],\n",
       "       [3.07507508],\n",
       "       [3.07907908],\n",
       "       [3.08308308],\n",
       "       [3.08708709],\n",
       "       [3.09109109],\n",
       "       [3.0950951 ],\n",
       "       [3.0990991 ],\n",
       "       [3.1031031 ],\n",
       "       [3.10710711],\n",
       "       [3.11111111],\n",
       "       [3.11511512],\n",
       "       [3.11911912],\n",
       "       [3.12312312],\n",
       "       [3.12712713],\n",
       "       [3.13113113],\n",
       "       [3.13513514],\n",
       "       [3.13913914],\n",
       "       [3.14314314],\n",
       "       [3.14714715],\n",
       "       [3.15115115],\n",
       "       [3.15515516],\n",
       "       [3.15915916],\n",
       "       [3.16316316],\n",
       "       [3.16716717],\n",
       "       [3.17117117],\n",
       "       [3.17517518],\n",
       "       [3.17917918],\n",
       "       [3.18318318],\n",
       "       [3.18718719],\n",
       "       [3.19119119],\n",
       "       [3.1951952 ],\n",
       "       [3.1991992 ],\n",
       "       [3.2032032 ],\n",
       "       [3.20720721],\n",
       "       [3.21121121],\n",
       "       [3.21521522],\n",
       "       [3.21921922],\n",
       "       [3.22322322],\n",
       "       [3.22722723],\n",
       "       [3.23123123],\n",
       "       [3.23523524],\n",
       "       [3.23923924],\n",
       "       [3.24324324],\n",
       "       [3.24724725],\n",
       "       [3.25125125],\n",
       "       [3.25525526],\n",
       "       [3.25925926],\n",
       "       [3.26326326],\n",
       "       [3.26726727],\n",
       "       [3.27127127],\n",
       "       [3.27527528],\n",
       "       [3.27927928],\n",
       "       [3.28328328],\n",
       "       [3.28728729],\n",
       "       [3.29129129],\n",
       "       [3.2952953 ],\n",
       "       [3.2992993 ],\n",
       "       [3.3033033 ],\n",
       "       [3.30730731],\n",
       "       [3.31131131],\n",
       "       [3.31531532],\n",
       "       [3.31931932],\n",
       "       [3.32332332],\n",
       "       [3.32732733],\n",
       "       [3.33133133],\n",
       "       [3.33533534],\n",
       "       [3.33933934],\n",
       "       [3.34334334],\n",
       "       [3.34734735],\n",
       "       [3.35135135],\n",
       "       [3.35535536],\n",
       "       [3.35935936],\n",
       "       [3.36336336],\n",
       "       [3.36736737],\n",
       "       [3.37137137],\n",
       "       [3.37537538],\n",
       "       [3.37937938],\n",
       "       [3.38338338],\n",
       "       [3.38738739],\n",
       "       [3.39139139],\n",
       "       [3.3953954 ],\n",
       "       [3.3993994 ],\n",
       "       [3.4034034 ],\n",
       "       [3.40740741],\n",
       "       [3.41141141],\n",
       "       [3.41541542],\n",
       "       [3.41941942],\n",
       "       [3.42342342],\n",
       "       [3.42742743],\n",
       "       [3.43143143],\n",
       "       [3.43543544],\n",
       "       [3.43943944],\n",
       "       [3.44344344],\n",
       "       [3.44744745],\n",
       "       [3.45145145],\n",
       "       [3.45545546],\n",
       "       [3.45945946],\n",
       "       [3.46346346],\n",
       "       [3.46746747],\n",
       "       [3.47147147],\n",
       "       [3.47547548],\n",
       "       [3.47947948],\n",
       "       [3.48348348],\n",
       "       [3.48748749],\n",
       "       [3.49149149],\n",
       "       [3.4954955 ],\n",
       "       [3.4994995 ],\n",
       "       [3.5035035 ],\n",
       "       [3.50750751],\n",
       "       [3.51151151],\n",
       "       [3.51551552],\n",
       "       [3.51951952],\n",
       "       [3.52352352],\n",
       "       [3.52752753],\n",
       "       [3.53153153],\n",
       "       [3.53553554],\n",
       "       [3.53953954],\n",
       "       [3.54354354],\n",
       "       [3.54754755],\n",
       "       [3.55155155],\n",
       "       [3.55555556],\n",
       "       [3.55955956],\n",
       "       [3.56356356],\n",
       "       [3.56756757],\n",
       "       [3.57157157],\n",
       "       [3.57557558],\n",
       "       [3.57957958],\n",
       "       [3.58358358],\n",
       "       [3.58758759],\n",
       "       [3.59159159],\n",
       "       [3.5955956 ],\n",
       "       [3.5995996 ],\n",
       "       [3.6036036 ],\n",
       "       [3.60760761],\n",
       "       [3.61161161],\n",
       "       [3.61561562],\n",
       "       [3.61961962],\n",
       "       [3.62362362],\n",
       "       [3.62762763],\n",
       "       [3.63163163],\n",
       "       [3.63563564],\n",
       "       [3.63963964],\n",
       "       [3.64364364],\n",
       "       [3.64764765],\n",
       "       [3.65165165],\n",
       "       [3.65565566],\n",
       "       [3.65965966],\n",
       "       [3.66366366],\n",
       "       [3.66766767],\n",
       "       [3.67167167],\n",
       "       [3.67567568],\n",
       "       [3.67967968],\n",
       "       [3.68368368],\n",
       "       [3.68768769],\n",
       "       [3.69169169],\n",
       "       [3.6956957 ],\n",
       "       [3.6996997 ],\n",
       "       [3.7037037 ],\n",
       "       [3.70770771],\n",
       "       [3.71171171],\n",
       "       [3.71571572],\n",
       "       [3.71971972],\n",
       "       [3.72372372],\n",
       "       [3.72772773],\n",
       "       [3.73173173],\n",
       "       [3.73573574],\n",
       "       [3.73973974],\n",
       "       [3.74374374],\n",
       "       [3.74774775],\n",
       "       [3.75175175],\n",
       "       [3.75575576],\n",
       "       [3.75975976],\n",
       "       [3.76376376],\n",
       "       [3.76776777],\n",
       "       [3.77177177],\n",
       "       [3.77577578],\n",
       "       [3.77977978],\n",
       "       [3.78378378],\n",
       "       [3.78778779],\n",
       "       [3.79179179],\n",
       "       [3.7957958 ],\n",
       "       [3.7997998 ],\n",
       "       [3.8038038 ],\n",
       "       [3.80780781],\n",
       "       [3.81181181],\n",
       "       [3.81581582],\n",
       "       [3.81981982],\n",
       "       [3.82382382],\n",
       "       [3.82782783],\n",
       "       [3.83183183],\n",
       "       [3.83583584],\n",
       "       [3.83983984],\n",
       "       [3.84384384],\n",
       "       [3.84784785],\n",
       "       [3.85185185],\n",
       "       [3.85585586],\n",
       "       [3.85985986],\n",
       "       [3.86386386],\n",
       "       [3.86786787],\n",
       "       [3.87187187],\n",
       "       [3.87587588],\n",
       "       [3.87987988],\n",
       "       [3.88388388],\n",
       "       [3.88788789],\n",
       "       [3.89189189],\n",
       "       [3.8958959 ],\n",
       "       [3.8998999 ],\n",
       "       [3.9039039 ],\n",
       "       [3.90790791],\n",
       "       [3.91191191],\n",
       "       [3.91591592],\n",
       "       [3.91991992],\n",
       "       [3.92392392],\n",
       "       [3.92792793],\n",
       "       [3.93193193],\n",
       "       [3.93593594],\n",
       "       [3.93993994],\n",
       "       [3.94394394],\n",
       "       [3.94794795],\n",
       "       [3.95195195],\n",
       "       [3.95595596],\n",
       "       [3.95995996],\n",
       "       [3.96396396],\n",
       "       [3.96796797],\n",
       "       [3.97197197],\n",
       "       [3.97597598],\n",
       "       [3.97997998],\n",
       "       [3.98398398],\n",
       "       [3.98798799],\n",
       "       [3.99199199],\n",
       "       [3.995996  ],\n",
       "       [4.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5a93d32e-accc-4eab-917c-a6259f3d7985",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probablity=clf.predict(x_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "2ef67991-06c4-4fbc-bfaa-1d94c9ddcd8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False, False, False, False, False, False, False, False, False,\n",
       "       False,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f4c9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_probablity=clf.predict_proba(x_new)  #will get probablity values by using predict.proba & will get 1 or 0 if used only predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37db0fd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[9.99250016e-01, 7.49984089e-04],\n",
       "       [9.99236900e-01, 7.63099595e-04],\n",
       "       [9.99223556e-01, 7.76444284e-04],\n",
       "       ...,\n",
       "       [4.09576617e-05, 9.99959042e-01],\n",
       "       [4.02532162e-05, 9.99959747e-01],\n",
       "       [3.95608863e-05, 9.99960439e-01]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probablity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fe972bcb-e90c-45ef-965b-05e70a4f35cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7.49984089e-04, 7.63099595e-04, 7.76444284e-04, 7.90022153e-04,\n",
       "       8.03837270e-04, 8.17893775e-04, 8.32195877e-04, 8.46747862e-04,\n",
       "       8.61554086e-04, 8.76618985e-04, 8.91947070e-04, 9.07542930e-04,\n",
       "       9.23411234e-04, 9.39556733e-04, 9.55984260e-04, 9.72698731e-04,\n",
       "       9.89705150e-04, 1.00700860e-03, 1.02461427e-03, 1.04252742e-03,\n",
       "       1.06075341e-03, 1.07929769e-03, 1.09816581e-03, 1.11736341e-03,\n",
       "       1.13689623e-03, 1.15677012e-03, 1.17699100e-03, 1.19756493e-03,\n",
       "       1.21849806e-03, 1.23979664e-03, 1.26146703e-03, 1.28351572e-03,\n",
       "       1.30594928e-03, 1.32877442e-03, 1.35199795e-03, 1.37562681e-03,\n",
       "       1.39966805e-03, 1.42412885e-03, 1.44901651e-03, 1.47433846e-03,\n",
       "       1.50010225e-03, 1.52631557e-03, 1.55298624e-03, 1.58012221e-03,\n",
       "       1.60773158e-03, 1.63582257e-03, 1.66440356e-03, 1.69348307e-03,\n",
       "       1.72306977e-03, 1.75317246e-03, 1.78380012e-03, 1.81496187e-03,\n",
       "       1.84666698e-03, 1.87892490e-03, 1.91174523e-03, 1.94513774e-03,\n",
       "       1.97911235e-03, 2.01367918e-03, 2.04884852e-03, 2.08463080e-03,\n",
       "       2.12103669e-03, 2.15807699e-03, 2.19576271e-03, 2.23410505e-03,\n",
       "       2.27311540e-03, 2.31280533e-03, 2.35318665e-03, 2.39427133e-03,\n",
       "       2.43607155e-03, 2.47859974e-03, 2.52186848e-03, 2.56589063e-03,\n",
       "       2.61067922e-03, 2.65624753e-03, 2.70260907e-03, 2.74977755e-03,\n",
       "       2.79776696e-03, 2.84659149e-03, 2.89626559e-03, 2.94680396e-03,\n",
       "       2.99822156e-03, 3.05053357e-03, 3.10375546e-03, 3.15790296e-03,\n",
       "       3.21299207e-03, 3.26903904e-03, 3.32606042e-03, 3.38407305e-03,\n",
       "       3.44309402e-03, 3.50314076e-03, 3.56423094e-03, 3.62638259e-03,\n",
       "       3.68961399e-03, 3.75394378e-03, 3.81939088e-03, 3.88597454e-03,\n",
       "       3.95371436e-03, 4.02263024e-03, 4.09274244e-03, 4.16407154e-03,\n",
       "       4.23663849e-03, 4.31046459e-03, 4.38557149e-03, 4.46198120e-03,\n",
       "       4.53971614e-03, 4.61879906e-03, 4.69925311e-03, 4.78110186e-03,\n",
       "       4.86436922e-03, 4.94907955e-03, 5.03525761e-03, 5.12292855e-03,\n",
       "       5.21211797e-03, 5.30285189e-03, 5.39515676e-03, 5.48905948e-03,\n",
       "       5.58458741e-03, 5.68176834e-03, 5.78063054e-03, 5.88120277e-03,\n",
       "       5.98351423e-03, 6.08759464e-03, 6.19347420e-03, 6.30118361e-03,\n",
       "       6.41075409e-03, 6.52221738e-03, 6.63560572e-03, 6.75095192e-03,\n",
       "       6.86828931e-03, 6.98765178e-03, 7.10907377e-03, 7.23259030e-03,\n",
       "       7.35823696e-03, 7.48604994e-03, 7.61606600e-03, 7.74832252e-03,\n",
       "       7.88285749e-03, 8.01970953e-03, 8.15891788e-03, 8.30052243e-03,\n",
       "       8.44456371e-03, 8.59108293e-03, 8.74012195e-03, 8.89172333e-03,\n",
       "       9.04593031e-03, 9.20278683e-03, 9.36233754e-03, 9.52462783e-03,\n",
       "       9.68970380e-03, 9.85761231e-03, 1.00284010e-02, 1.02021181e-02,\n",
       "       1.03788130e-02, 1.05585355e-02, 1.07413363e-02, 1.09272670e-02,\n",
       "       1.11163799e-02, 1.13087283e-02, 1.15043663e-02, 1.17033486e-02,\n",
       "       1.19057312e-02, 1.21115706e-02, 1.23209244e-02, 1.25338511e-02,\n",
       "       1.27504100e-02, 1.29706614e-02, 1.31946667e-02, 1.34224879e-02,\n",
       "       1.36541883e-02, 1.38898320e-02, 1.41294842e-02, 1.43732111e-02,\n",
       "       1.46210797e-02, 1.48731584e-02, 1.51295164e-02, 1.53902241e-02,\n",
       "       1.56553527e-02, 1.59249749e-02, 1.61991642e-02, 1.64779953e-02,\n",
       "       1.67615441e-02, 1.70498876e-02, 1.73431038e-02, 1.76412721e-02,\n",
       "       1.79444730e-02, 1.82527882e-02, 1.85663006e-02, 1.88850944e-02,\n",
       "       1.92092548e-02, 1.95388687e-02, 1.98740238e-02, 2.02148094e-02,\n",
       "       2.05613160e-02, 2.09136353e-02, 2.12718605e-02, 2.16360861e-02,\n",
       "       2.20064079e-02, 2.23829231e-02, 2.27657302e-02, 2.31549293e-02,\n",
       "       2.35506218e-02, 2.39529104e-02, 2.43618993e-02, 2.47776944e-02,\n",
       "       2.52004027e-02, 2.56301329e-02, 2.60669951e-02, 2.65111010e-02,\n",
       "       2.69625637e-02, 2.74214979e-02, 2.78880198e-02, 2.83622471e-02,\n",
       "       2.88442993e-02, 2.93342972e-02, 2.98323634e-02, 3.03386219e-02,\n",
       "       3.08531984e-02, 3.13762203e-02, 3.19078165e-02, 3.24481177e-02,\n",
       "       3.29972560e-02, 3.35553655e-02, 3.41225816e-02, 3.46990416e-02,\n",
       "       3.52848844e-02, 3.58802507e-02, 3.64852827e-02, 3.71001245e-02,\n",
       "       3.77249219e-02, 3.83598221e-02, 3.90049745e-02, 3.96605298e-02,\n",
       "       4.03266406e-02, 4.10034613e-02, 4.16911479e-02, 4.23898582e-02,\n",
       "       4.30997517e-02, 4.38209896e-02, 4.45537348e-02, 4.52981521e-02,\n",
       "       4.60544078e-02, 4.68226700e-02, 4.76031085e-02, 4.83958948e-02,\n",
       "       4.92012023e-02, 5.00192056e-02, 5.08500815e-02, 5.16940082e-02,\n",
       "       5.25511655e-02, 5.34217350e-02, 5.43058998e-02, 5.52038447e-02,\n",
       "       5.61157560e-02, 5.70418217e-02, 5.79822313e-02, 5.89371757e-02,\n",
       "       5.99068475e-02, 6.08914407e-02, 6.18911507e-02, 6.29061744e-02,\n",
       "       6.39367101e-02, 6.49829574e-02, 6.60451174e-02, 6.71233922e-02,\n",
       "       6.82179855e-02, 6.93291020e-02, 7.04569476e-02, 7.16017293e-02,\n",
       "       7.27636554e-02, 7.39429351e-02, 7.51397785e-02, 7.63543969e-02,\n",
       "       7.75870021e-02, 7.88378072e-02, 8.01070257e-02, 8.13948720e-02,\n",
       "       8.27015610e-02, 8.40273085e-02, 8.53723303e-02, 8.67368432e-02,\n",
       "       8.81210639e-02, 8.95252097e-02, 9.09494980e-02, 9.23941464e-02,\n",
       "       9.38593724e-02, 9.53453936e-02, 9.68524274e-02, 9.83806910e-02,\n",
       "       9.99304013e-02, 1.01501775e-01, 1.03095027e-01, 1.04710374e-01,\n",
       "       1.06348030e-01, 1.08008209e-01, 1.09691123e-01, 1.11396985e-01,\n",
       "       1.13126005e-01, 1.14878391e-01, 1.16654353e-01, 1.18454096e-01,\n",
       "       1.20277824e-01, 1.22125741e-01, 1.23998047e-01, 1.25894941e-01,\n",
       "       1.27816619e-01, 1.29763276e-01, 1.31735102e-01, 1.33732286e-01,\n",
       "       1.35755015e-01, 1.37803471e-01, 1.39877835e-01, 1.41978282e-01,\n",
       "       1.44104985e-01, 1.46258115e-01, 1.48437836e-01, 1.50644310e-01,\n",
       "       1.52877694e-01, 1.55138141e-01, 1.57425800e-01, 1.59740815e-01,\n",
       "       1.62083324e-01, 1.64453461e-01, 1.66851356e-01, 1.69277130e-01,\n",
       "       1.71730903e-01, 1.74212785e-01, 1.76722883e-01, 1.79261295e-01,\n",
       "       1.81828117e-01, 1.84423433e-01, 1.87047325e-01, 1.89699865e-01,\n",
       "       1.92381120e-01, 1.95091147e-01, 1.97829999e-01, 2.00597719e-01,\n",
       "       2.03394342e-01, 2.06219896e-01, 2.09074401e-01, 2.11957867e-01,\n",
       "       2.14870297e-01, 2.17811684e-01, 2.20782014e-01, 2.23781262e-01,\n",
       "       2.26809395e-01, 2.29866368e-01, 2.32952130e-01, 2.36066619e-01,\n",
       "       2.39209761e-01, 2.42381475e-01, 2.45581669e-01, 2.48810238e-01,\n",
       "       2.52067071e-01, 2.55352043e-01, 2.58665020e-01, 2.62005856e-01,\n",
       "       2.65374395e-01, 2.68770470e-01, 2.72193903e-01, 2.75644504e-01,\n",
       "       2.79122072e-01, 2.82626395e-01, 2.86157249e-01, 2.89714400e-01,\n",
       "       2.93297602e-01, 2.96906595e-01, 3.00541111e-01, 3.04200869e-01,\n",
       "       3.07885575e-01, 3.11594927e-01, 3.15328607e-01, 3.19086289e-01,\n",
       "       3.22867633e-01, 3.26672291e-01, 3.30499899e-01, 3.34350086e-01,\n",
       "       3.38222466e-01, 3.42116645e-01, 3.46032216e-01, 3.49968762e-01,\n",
       "       3.53925854e-01, 3.57903055e-01, 3.61899913e-01, 3.65915970e-01,\n",
       "       3.69950755e-01, 3.74003788e-01, 3.78074580e-01, 3.82162630e-01,\n",
       "       3.86267429e-01, 3.90388460e-01, 3.94525194e-01, 3.98677095e-01,\n",
       "       4.02843620e-01, 4.07024214e-01, 4.11218317e-01, 4.15425360e-01,\n",
       "       4.19644768e-01, 4.23875956e-01, 4.28118335e-01, 4.32371309e-01,\n",
       "       4.36634275e-01, 4.40906624e-01, 4.45187742e-01, 4.49477011e-01,\n",
       "       4.53773805e-01, 4.58077497e-01, 4.62387454e-01, 4.66703039e-01,\n",
       "       4.71023613e-01, 4.75348534e-01, 4.79677155e-01, 4.84008830e-01,\n",
       "       4.88342909e-01, 4.92678742e-01, 4.97015676e-01, 5.01353059e-01,\n",
       "       5.05690239e-01, 5.10026562e-01, 5.14361377e-01, 5.18694033e-01,\n",
       "       5.23023878e-01, 5.27350267e-01, 5.31672551e-01, 5.35990088e-01,\n",
       "       5.40302236e-01, 5.44608359e-01, 5.48907821e-01, 5.53199993e-01,\n",
       "       5.57484250e-01, 5.61759969e-01, 5.66026536e-01, 5.70283339e-01,\n",
       "       5.74529773e-01, 5.78765240e-01, 5.82989147e-01, 5.87200908e-01,\n",
       "       5.91399945e-01, 5.95585686e-01, 5.99757568e-01, 6.03915033e-01,\n",
       "       6.08057535e-01, 6.12184534e-01, 6.16295499e-01, 6.20389909e-01,\n",
       "       6.24467250e-01, 6.28527021e-01, 6.32568726e-01, 6.36591882e-01,\n",
       "       6.40596015e-01, 6.44580662e-01, 6.48545369e-01, 6.52489693e-01,\n",
       "       6.56413203e-01, 6.60315475e-01, 6.64196101e-01, 6.68054681e-01,\n",
       "       6.71890825e-01, 6.75704157e-01, 6.79494311e-01, 6.83260933e-01,\n",
       "       6.87003679e-01, 6.90722218e-01, 6.94416230e-01, 6.98085406e-01,\n",
       "       7.01729449e-01, 7.05348074e-01, 7.08941006e-01, 7.12507984e-01,\n",
       "       7.16048757e-01, 7.19563085e-01, 7.23050741e-01, 7.26511509e-01,\n",
       "       7.29945182e-01, 7.33351568e-01, 7.36730483e-01, 7.40081758e-01,\n",
       "       7.43405230e-01, 7.46700751e-01, 7.49968182e-01, 7.53207395e-01,\n",
       "       7.56418273e-01, 7.59600710e-01, 7.62754608e-01, 7.65879882e-01,\n",
       "       7.68976456e-01, 7.72044264e-01, 7.75083249e-01, 7.78093365e-01,\n",
       "       7.81074574e-01, 7.84026849e-01, 7.86950171e-01, 7.89844531e-01,\n",
       "       7.92709927e-01, 7.95546367e-01, 7.98353869e-01, 8.01132455e-01,\n",
       "       8.03882159e-01, 8.06603022e-01, 8.09295091e-01, 8.11958424e-01,\n",
       "       8.14593082e-01, 8.17199137e-01, 8.19776665e-01, 8.22325752e-01,\n",
       "       8.24846487e-01, 8.27338969e-01, 8.29803301e-01, 8.32239592e-01,\n",
       "       8.34647958e-01, 8.37028520e-01, 8.39381405e-01, 8.41706744e-01,\n",
       "       8.44004675e-01, 8.46275339e-01, 8.48518884e-01, 8.50735460e-01,\n",
       "       8.52925224e-01, 8.55088335e-01, 8.57224958e-01, 8.59335260e-01,\n",
       "       8.61419413e-01, 8.63477592e-01, 8.65509976e-01, 8.67516746e-01,\n",
       "       8.69498087e-01, 8.71454187e-01, 8.73385237e-01, 8.75291429e-01,\n",
       "       8.77172960e-01, 8.79030026e-01, 8.80862828e-01, 8.82671568e-01,\n",
       "       8.84456449e-01, 8.86217678e-01, 8.87955462e-01, 8.89670008e-01,\n",
       "       8.91361528e-01, 8.93030233e-01, 8.94676335e-01, 8.96300047e-01,\n",
       "       8.97901584e-01, 8.99481161e-01, 9.01038993e-01, 9.02575298e-01,\n",
       "       9.04090290e-01, 9.05584189e-01, 9.07057210e-01, 9.08509573e-01,\n",
       "       9.09941493e-01, 9.11353189e-01, 9.12744878e-01, 9.14116777e-01,\n",
       "       9.15469104e-01, 9.16802075e-01, 9.18115907e-01, 9.19410814e-01,\n",
       "       9.20687014e-01, 9.21944719e-01, 9.23184144e-01, 9.24405502e-01,\n",
       "       9.25609006e-01, 9.26794867e-01, 9.27963295e-01, 9.29114500e-01,\n",
       "       9.30248691e-01, 9.31366075e-01, 9.32466859e-01, 9.33551247e-01,\n",
       "       9.34619444e-01, 9.35671652e-01, 9.36708073e-01, 9.37728908e-01,\n",
       "       9.38734354e-01, 9.39724610e-01, 9.40699871e-01, 9.41660331e-01,\n",
       "       9.42606185e-01, 9.43537623e-01, 9.44454836e-01, 9.45358011e-01,\n",
       "       9.46247337e-01, 9.47122998e-01, 9.47985178e-01, 9.48834060e-01,\n",
       "       9.49669823e-01, 9.50492647e-01, 9.51302709e-01, 9.52100184e-01,\n",
       "       9.52885246e-01, 9.53658068e-01, 9.54418819e-01, 9.55167669e-01,\n",
       "       9.55904785e-01, 9.56630332e-01, 9.57344473e-01, 9.58047370e-01,\n",
       "       9.58739184e-01, 9.59420073e-01, 9.60090194e-01, 9.60749701e-01,\n",
       "       9.61398748e-01, 9.62037487e-01, 9.62666066e-01, 9.63284635e-01,\n",
       "       9.63893340e-01, 9.64492324e-01, 9.65081732e-01, 9.65661705e-01,\n",
       "       9.66232381e-01, 9.66793900e-01, 9.67346396e-01, 9.67890005e-01,\n",
       "       9.68424860e-01, 9.68951092e-01, 9.69468829e-01, 9.69978202e-01,\n",
       "       9.70479334e-01, 9.70972352e-01, 9.71457378e-01, 9.71934535e-01,\n",
       "       9.72403941e-01, 9.72865715e-01, 9.73319974e-01, 9.73766833e-01,\n",
       "       9.74206407e-01, 9.74638806e-01, 9.75064143e-01, 9.75482525e-01,\n",
       "       9.75894061e-01, 9.76298858e-01, 9.76697019e-01, 9.77088648e-01,\n",
       "       9.77473847e-01, 9.77852717e-01, 9.78225357e-01, 9.78591864e-01,\n",
       "       9.78952335e-01, 9.79306864e-01, 9.79655546e-01, 9.79998473e-01,\n",
       "       9.80335735e-01, 9.80667422e-01, 9.80993624e-01, 9.81314426e-01,\n",
       "       9.81629915e-01, 9.81940175e-01, 9.82245289e-01, 9.82545341e-01,\n",
       "       9.82840410e-01, 9.83130577e-01, 9.83415920e-01, 9.83696517e-01,\n",
       "       9.83972443e-01, 9.84243774e-01, 9.84510584e-01, 9.84772946e-01,\n",
       "       9.85030932e-01, 9.85284612e-01, 9.85534056e-01, 9.85779332e-01,\n",
       "       9.86020509e-01, 9.86257653e-01, 9.86490829e-01, 9.86720101e-01,\n",
       "       9.86945534e-01, 9.87167190e-01, 9.87385131e-01, 9.87599417e-01,\n",
       "       9.87810107e-01, 9.88017262e-01, 9.88220938e-01, 9.88421192e-01,\n",
       "       9.88618081e-01, 9.88811661e-01, 9.89001984e-01, 9.89189106e-01,\n",
       "       9.89373077e-01, 9.89553952e-01, 9.89731779e-01, 9.89906610e-01,\n",
       "       9.90078495e-01, 9.90247481e-01, 9.90413617e-01, 9.90576949e-01,\n",
       "       9.90737525e-01, 9.90895389e-01, 9.91050588e-01, 9.91203164e-01,\n",
       "       9.91353162e-01, 9.91500624e-01, 9.91645592e-01, 9.91788108e-01,\n",
       "       9.91928213e-01, 9.92065947e-01, 9.92201348e-01, 9.92334457e-01,\n",
       "       9.92465312e-01, 9.92593949e-01, 9.92720406e-01, 9.92844720e-01,\n",
       "       9.92966925e-01, 9.93087058e-01, 9.93205153e-01, 9.93321244e-01,\n",
       "       9.93435365e-01, 9.93547549e-01, 9.93657827e-01, 9.93766233e-01,\n",
       "       9.93872797e-01, 9.93977550e-01, 9.94080524e-01, 9.94181747e-01,\n",
       "       9.94281249e-01, 9.94379058e-01, 9.94475205e-01, 9.94569716e-01,\n",
       "       9.94662618e-01, 9.94753940e-01, 9.94843707e-01, 9.94931946e-01,\n",
       "       9.95018683e-01, 9.95103942e-01, 9.95187749e-01, 9.95270129e-01,\n",
       "       9.95351104e-01, 9.95430700e-01, 9.95508940e-01, 9.95585845e-01,\n",
       "       9.95661439e-01, 9.95735745e-01, 9.95808783e-01, 9.95880575e-01,\n",
       "       9.95951142e-01, 9.96020506e-01, 9.96088685e-01, 9.96155702e-01,\n",
       "       9.96221574e-01, 9.96286322e-01, 9.96349964e-01, 9.96412519e-01,\n",
       "       9.96474007e-01, 9.96534444e-01, 9.96593848e-01, 9.96652238e-01,\n",
       "       9.96709631e-01, 9.96766042e-01, 9.96821489e-01, 9.96875989e-01,\n",
       "       9.96929557e-01, 9.96982210e-01, 9.97033962e-01, 9.97084829e-01,\n",
       "       9.97134827e-01, 9.97183969e-01, 9.97232271e-01, 9.97279746e-01,\n",
       "       9.97326410e-01, 9.97372275e-01, 9.97417355e-01, 9.97461664e-01,\n",
       "       9.97505215e-01, 9.97548020e-01, 9.97590092e-01, 9.97631445e-01,\n",
       "       9.97672089e-01, 9.97712038e-01, 9.97751302e-01, 9.97789894e-01,\n",
       "       9.97827826e-01, 9.97865108e-01, 9.97901751e-01, 9.97937766e-01,\n",
       "       9.97973165e-01, 9.98007957e-01, 9.98042153e-01, 9.98075764e-01,\n",
       "       9.98108798e-01, 9.98141266e-01, 9.98173178e-01, 9.98204543e-01,\n",
       "       9.98235371e-01, 9.98265670e-01, 9.98295449e-01, 9.98324719e-01,\n",
       "       9.98353486e-01, 9.98381760e-01, 9.98409550e-01, 9.98436863e-01,\n",
       "       9.98463708e-01, 9.98490092e-01, 9.98516024e-01, 9.98541511e-01,\n",
       "       9.98566561e-01, 9.98591182e-01, 9.98615380e-01, 9.98639163e-01,\n",
       "       9.98662538e-01, 9.98685513e-01, 9.98708093e-01, 9.98730285e-01,\n",
       "       9.98752097e-01, 9.98773535e-01, 9.98794605e-01, 9.98815313e-01,\n",
       "       9.98835666e-01, 9.98855670e-01, 9.98875330e-01, 9.98894653e-01,\n",
       "       9.98913644e-01, 9.98932310e-01, 9.98950655e-01, 9.98968685e-01,\n",
       "       9.98986406e-01, 9.99003822e-01, 9.99020940e-01, 9.99037763e-01,\n",
       "       9.99054298e-01, 9.99070549e-01, 9.99086521e-01, 9.99102219e-01,\n",
       "       9.99117647e-01, 9.99132811e-01, 9.99147714e-01, 9.99162361e-01,\n",
       "       9.99176756e-01, 9.99190905e-01, 9.99204810e-01, 9.99218477e-01,\n",
       "       9.99231909e-01, 9.99245110e-01, 9.99258084e-01, 9.99270836e-01,\n",
       "       9.99283369e-01, 9.99295686e-01, 9.99307792e-01, 9.99319690e-01,\n",
       "       9.99331383e-01, 9.99342876e-01, 9.99354171e-01, 9.99365273e-01,\n",
       "       9.99376183e-01, 9.99386906e-01, 9.99397445e-01, 9.99407803e-01,\n",
       "       9.99417983e-01, 9.99427988e-01, 9.99437821e-01, 9.99447485e-01,\n",
       "       9.99456983e-01, 9.99466318e-01, 9.99475493e-01, 9.99484510e-01,\n",
       "       9.99493372e-01, 9.99502081e-01, 9.99510641e-01, 9.99519054e-01,\n",
       "       9.99527323e-01, 9.99535449e-01, 9.99543436e-01, 9.99551285e-01,\n",
       "       9.99558999e-01, 9.99566581e-01, 9.99574033e-01, 9.99581357e-01,\n",
       "       9.99588554e-01, 9.99595628e-01, 9.99602581e-01, 9.99609414e-01,\n",
       "       9.99616129e-01, 9.99622729e-01, 9.99629216e-01, 9.99635591e-01,\n",
       "       9.99641857e-01, 9.99648015e-01, 9.99654067e-01, 9.99660015e-01,\n",
       "       9.99665861e-01, 9.99671606e-01, 9.99677253e-01, 9.99682802e-01,\n",
       "       9.99688256e-01, 9.99693617e-01, 9.99698885e-01, 9.99704062e-01,\n",
       "       9.99709151e-01, 9.99714152e-01, 9.99719068e-01, 9.99723898e-01,\n",
       "       9.99728646e-01, 9.99733312e-01, 9.99737898e-01, 9.99742405e-01,\n",
       "       9.99746834e-01, 9.99751188e-01, 9.99755466e-01, 9.99759671e-01,\n",
       "       9.99763804e-01, 9.99767866e-01, 9.99771857e-01, 9.99775781e-01,\n",
       "       9.99779636e-01, 9.99783426e-01, 9.99787150e-01, 9.99790810e-01,\n",
       "       9.99794408e-01, 9.99797943e-01, 9.99801418e-01, 9.99804833e-01,\n",
       "       9.99808189e-01, 9.99811488e-01, 9.99814729e-01, 9.99817915e-01,\n",
       "       9.99821047e-01, 9.99824124e-01, 9.99827149e-01, 9.99830121e-01,\n",
       "       9.99833043e-01, 9.99835914e-01, 9.99838736e-01, 9.99841509e-01,\n",
       "       9.99844235e-01, 9.99846914e-01, 9.99849546e-01, 9.99852134e-01,\n",
       "       9.99854677e-01, 9.99857176e-01, 9.99859632e-01, 9.99862046e-01,\n",
       "       9.99864419e-01, 9.99866750e-01, 9.99869042e-01, 9.99871294e-01,\n",
       "       9.99873507e-01, 9.99875683e-01, 9.99877821e-01, 9.99879922e-01,\n",
       "       9.99881987e-01, 9.99884017e-01, 9.99886012e-01, 9.99887972e-01,\n",
       "       9.99889899e-01, 9.99891792e-01, 9.99893653e-01, 9.99895482e-01,\n",
       "       9.99897280e-01, 9.99899046e-01, 9.99900782e-01, 9.99902489e-01,\n",
       "       9.99904166e-01, 9.99905814e-01, 9.99907434e-01, 9.99909026e-01,\n",
       "       9.99910591e-01, 9.99912128e-01, 9.99913640e-01, 9.99915125e-01,\n",
       "       9.99916585e-01, 9.99918019e-01, 9.99919429e-01, 9.99920815e-01,\n",
       "       9.99922177e-01, 9.99923515e-01, 9.99924831e-01, 9.99926123e-01,\n",
       "       9.99927394e-01, 9.99928643e-01, 9.99929870e-01, 9.99931076e-01,\n",
       "       9.99932262e-01, 9.99933427e-01, 9.99934572e-01, 9.99935697e-01,\n",
       "       9.99936803e-01, 9.99937890e-01, 9.99938958e-01, 9.99940008e-01,\n",
       "       9.99941040e-01, 9.99942054e-01, 9.99943050e-01, 9.99944030e-01,\n",
       "       9.99944993e-01, 9.99945939e-01, 9.99946868e-01, 9.99947782e-01,\n",
       "       9.99948680e-01, 9.99949563e-01, 9.99950430e-01, 9.99951283e-01,\n",
       "       9.99952121e-01, 9.99952944e-01, 9.99953754e-01, 9.99954549e-01,\n",
       "       9.99955331e-01, 9.99956099e-01, 9.99956854e-01, 9.99957596e-01,\n",
       "       9.99958326e-01, 9.99959042e-01, 9.99959747e-01, 9.99960439e-01])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_probablity[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "77716c33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_new, y_probablity[:,1], \"blue\", label=\"setosa\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3938549-f508-47f1-a6c0-e795c89c471a",
   "metadata": {},
   "source": [
    "###########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1475560d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import datasets\n",
    "df=datasets.load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c23b9db8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n|details-start|\\n**References**\\n|details-split|\\n\\n- W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n  for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n  Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n  San Jose, CA, 1993.\\n- O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n  prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n  July-August 1995.\\n- W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n  to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n  163-171.\\n\\n|details-end|',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'breast_cancer.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2aa9f232",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=pd.DataFrame(df['data'],columns=df['feature_names'])    #independent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b1a17046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.380</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.990</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.570</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.540</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>25.450</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>23.690</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>18.980</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>25.740</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>9.456</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "0                   0.07871  ...        25.380          17.33   \n",
       "1                   0.05667  ...        24.990          23.41   \n",
       "2                   0.05999  ...        23.570          25.53   \n",
       "3                   0.09744  ...        14.910          26.50   \n",
       "4                   0.05883  ...        22.540          16.67   \n",
       "..                      ...  ...           ...            ...   \n",
       "564                 0.05623  ...        25.450          26.40   \n",
       "565                 0.05533  ...        23.690          38.25   \n",
       "566                 0.05648  ...        18.980          34.12   \n",
       "567                 0.07016  ...        25.740          39.42   \n",
       "568                 0.05884  ...         9.456          30.37   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "0             184.60      2019.0           0.16220            0.66560   \n",
       "1             158.80      1956.0           0.12380            0.18660   \n",
       "2             152.50      1709.0           0.14440            0.42450   \n",
       "3              98.87       567.7           0.20980            0.86630   \n",
       "4             152.20      1575.0           0.13740            0.20500   \n",
       "..               ...         ...               ...                ...   \n",
       "564           166.10      2027.0           0.14100            0.21130   \n",
       "565           155.00      1731.0           0.11660            0.19220   \n",
       "566           126.70      1124.0           0.11390            0.30940   \n",
       "567           184.60      1821.0           0.16500            0.86810   \n",
       "568            59.16       268.6           0.08996            0.06444   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "0             0.7119                0.2654          0.4601   \n",
       "1             0.2416                0.1860          0.2750   \n",
       "2             0.4504                0.2430          0.3613   \n",
       "3             0.6869                0.2575          0.6638   \n",
       "4             0.4000                0.1625          0.2364   \n",
       "..               ...                   ...             ...   \n",
       "564           0.4107                0.2216          0.2060   \n",
       "565           0.3215                0.1628          0.2572   \n",
       "566           0.3403                0.1418          0.2218   \n",
       "567           0.9387                0.2650          0.4087   \n",
       "568           0.0000                0.0000          0.2871   \n",
       "\n",
       "     worst fractal dimension  \n",
       "0                    0.11890  \n",
       "1                    0.08902  \n",
       "2                    0.08758  \n",
       "3                    0.17300  \n",
       "4                    0.07678  \n",
       "..                       ...  \n",
       "564                  0.07115  \n",
       "565                  0.06637  \n",
       "566                  0.07820  \n",
       "567                  0.12400  \n",
       "568                  0.07039  \n",
       "\n",
       "[569 rows x 30 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "10ef31a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y=pd.DataFrame(df['target'],columns=['Target'])    #dependent feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6df9919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target\n",
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "..      ...\n",
       "564       0\n",
       "565       0\n",
       "566       0\n",
       "567       0\n",
       "568       1\n",
       "\n",
       "[569 rows x 1 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b13abe11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Target\n",
       "1    357\n",
       "0    212\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y['Target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0f816dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_train, x_test, y_train, y_test=train_test_split(x,y,test_size=0.30,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "34e69992-4ad9-4c87-9295-d675c8134253",
   "metadata": {},
   "outputs": [],
   "source": [
    "params=[{'C':[1,5,10]},{'max_iter':[100,150]}]\n",
    "model1=LogisticRegression(C=100,max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d1b14997",
   "metadata": {},
   "outputs": [],
   "source": [
    "model=GridSearchCV(model1,param_grid=params,scoring='f1',cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5666bf87",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\Users\\HUAWEI\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(C=100),\n",
       "             param_grid=[{&#x27;C&#x27;: [1, 5, 10]}, {&#x27;max_iter&#x27;: [100, 150]}],\n",
       "             scoring=&#x27;f1&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=LogisticRegression(C=100),\n",
       "             param_grid=[{&#x27;C&#x27;: [1, 5, 10]}, {&#x27;max_iter&#x27;: [100, 150]}],\n",
       "             scoring=&#x27;f1&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(C=100)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=LogisticRegression(C=100),\n",
       "             param_grid=[{'C': [1, 5, 10]}, {'max_iter': [100, 150]}],\n",
       "             scoring='f1')"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "e7fb257c-4ced-48f0-85fb-9e69210a58b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>13.740</td>\n",
       "      <td>17.91</td>\n",
       "      <td>88.12</td>\n",
       "      <td>585.0</td>\n",
       "      <td>0.07944</td>\n",
       "      <td>0.06376</td>\n",
       "      <td>0.02881</td>\n",
       "      <td>0.01329</td>\n",
       "      <td>0.1473</td>\n",
       "      <td>0.05580</td>\n",
       "      <td>...</td>\n",
       "      <td>15.340</td>\n",
       "      <td>22.46</td>\n",
       "      <td>97.19</td>\n",
       "      <td>725.9</td>\n",
       "      <td>0.09711</td>\n",
       "      <td>0.18240</td>\n",
       "      <td>0.15640</td>\n",
       "      <td>0.06019</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.07014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>13.370</td>\n",
       "      <td>16.39</td>\n",
       "      <td>86.10</td>\n",
       "      <td>553.5</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0.07325</td>\n",
       "      <td>0.08092</td>\n",
       "      <td>0.02800</td>\n",
       "      <td>0.1422</td>\n",
       "      <td>0.05823</td>\n",
       "      <td>...</td>\n",
       "      <td>14.260</td>\n",
       "      <td>22.75</td>\n",
       "      <td>91.99</td>\n",
       "      <td>632.1</td>\n",
       "      <td>0.10250</td>\n",
       "      <td>0.25310</td>\n",
       "      <td>0.33080</td>\n",
       "      <td>0.08978</td>\n",
       "      <td>0.2048</td>\n",
       "      <td>0.07628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>14.690</td>\n",
       "      <td>13.98</td>\n",
       "      <td>98.22</td>\n",
       "      <td>656.1</td>\n",
       "      <td>0.10310</td>\n",
       "      <td>0.18360</td>\n",
       "      <td>0.14500</td>\n",
       "      <td>0.06300</td>\n",
       "      <td>0.2086</td>\n",
       "      <td>0.07406</td>\n",
       "      <td>...</td>\n",
       "      <td>16.460</td>\n",
       "      <td>18.34</td>\n",
       "      <td>114.10</td>\n",
       "      <td>809.2</td>\n",
       "      <td>0.13120</td>\n",
       "      <td>0.36350</td>\n",
       "      <td>0.32190</td>\n",
       "      <td>0.11080</td>\n",
       "      <td>0.2827</td>\n",
       "      <td>0.09208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>12.910</td>\n",
       "      <td>16.33</td>\n",
       "      <td>82.53</td>\n",
       "      <td>516.4</td>\n",
       "      <td>0.07941</td>\n",
       "      <td>0.05366</td>\n",
       "      <td>0.03873</td>\n",
       "      <td>0.02377</td>\n",
       "      <td>0.1829</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>13.880</td>\n",
       "      <td>22.00</td>\n",
       "      <td>90.81</td>\n",
       "      <td>600.6</td>\n",
       "      <td>0.10970</td>\n",
       "      <td>0.15060</td>\n",
       "      <td>0.17640</td>\n",
       "      <td>0.08235</td>\n",
       "      <td>0.3024</td>\n",
       "      <td>0.06949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>13.620</td>\n",
       "      <td>23.23</td>\n",
       "      <td>87.19</td>\n",
       "      <td>573.2</td>\n",
       "      <td>0.09246</td>\n",
       "      <td>0.06747</td>\n",
       "      <td>0.02974</td>\n",
       "      <td>0.02443</td>\n",
       "      <td>0.1664</td>\n",
       "      <td>0.05801</td>\n",
       "      <td>...</td>\n",
       "      <td>15.350</td>\n",
       "      <td>29.09</td>\n",
       "      <td>97.58</td>\n",
       "      <td>729.8</td>\n",
       "      <td>0.12160</td>\n",
       "      <td>0.15170</td>\n",
       "      <td>0.10490</td>\n",
       "      <td>0.07174</td>\n",
       "      <td>0.2642</td>\n",
       "      <td>0.06953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>8.888</td>\n",
       "      <td>14.64</td>\n",
       "      <td>58.79</td>\n",
       "      <td>244.0</td>\n",
       "      <td>0.09783</td>\n",
       "      <td>0.15310</td>\n",
       "      <td>0.08606</td>\n",
       "      <td>0.02872</td>\n",
       "      <td>0.1902</td>\n",
       "      <td>0.08980</td>\n",
       "      <td>...</td>\n",
       "      <td>9.733</td>\n",
       "      <td>15.67</td>\n",
       "      <td>62.56</td>\n",
       "      <td>284.4</td>\n",
       "      <td>0.12070</td>\n",
       "      <td>0.24360</td>\n",
       "      <td>0.14340</td>\n",
       "      <td>0.04786</td>\n",
       "      <td>0.2254</td>\n",
       "      <td>0.10840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>11.640</td>\n",
       "      <td>18.33</td>\n",
       "      <td>75.17</td>\n",
       "      <td>412.5</td>\n",
       "      <td>0.11420</td>\n",
       "      <td>0.10170</td>\n",
       "      <td>0.07070</td>\n",
       "      <td>0.03485</td>\n",
       "      <td>0.1801</td>\n",
       "      <td>0.06520</td>\n",
       "      <td>...</td>\n",
       "      <td>13.140</td>\n",
       "      <td>29.26</td>\n",
       "      <td>85.51</td>\n",
       "      <td>521.7</td>\n",
       "      <td>0.16880</td>\n",
       "      <td>0.26600</td>\n",
       "      <td>0.28730</td>\n",
       "      <td>0.12180</td>\n",
       "      <td>0.2806</td>\n",
       "      <td>0.09097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>14.290</td>\n",
       "      <td>16.82</td>\n",
       "      <td>90.30</td>\n",
       "      <td>632.6</td>\n",
       "      <td>0.06429</td>\n",
       "      <td>0.02675</td>\n",
       "      <td>0.00725</td>\n",
       "      <td>0.00625</td>\n",
       "      <td>0.1508</td>\n",
       "      <td>0.05376</td>\n",
       "      <td>...</td>\n",
       "      <td>14.910</td>\n",
       "      <td>20.65</td>\n",
       "      <td>94.44</td>\n",
       "      <td>684.6</td>\n",
       "      <td>0.08567</td>\n",
       "      <td>0.05036</td>\n",
       "      <td>0.03866</td>\n",
       "      <td>0.03333</td>\n",
       "      <td>0.2458</td>\n",
       "      <td>0.06120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>13.980</td>\n",
       "      <td>19.62</td>\n",
       "      <td>91.12</td>\n",
       "      <td>599.5</td>\n",
       "      <td>0.10600</td>\n",
       "      <td>0.11330</td>\n",
       "      <td>0.11260</td>\n",
       "      <td>0.06463</td>\n",
       "      <td>0.1669</td>\n",
       "      <td>0.06544</td>\n",
       "      <td>...</td>\n",
       "      <td>17.040</td>\n",
       "      <td>30.80</td>\n",
       "      <td>113.90</td>\n",
       "      <td>869.3</td>\n",
       "      <td>0.16130</td>\n",
       "      <td>0.35680</td>\n",
       "      <td>0.40690</td>\n",
       "      <td>0.18270</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>0.10550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>12.180</td>\n",
       "      <td>20.52</td>\n",
       "      <td>77.22</td>\n",
       "      <td>458.7</td>\n",
       "      <td>0.08013</td>\n",
       "      <td>0.04038</td>\n",
       "      <td>0.02383</td>\n",
       "      <td>0.01770</td>\n",
       "      <td>0.1739</td>\n",
       "      <td>0.05677</td>\n",
       "      <td>...</td>\n",
       "      <td>13.340</td>\n",
       "      <td>32.84</td>\n",
       "      <td>84.58</td>\n",
       "      <td>547.8</td>\n",
       "      <td>0.11230</td>\n",
       "      <td>0.08862</td>\n",
       "      <td>0.11450</td>\n",
       "      <td>0.07431</td>\n",
       "      <td>0.2694</td>\n",
       "      <td>0.06878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "149       13.740         17.91           88.12      585.0          0.07944   \n",
       "124       13.370         16.39           86.10      553.5          0.07115   \n",
       "421       14.690         13.98           98.22      656.1          0.10310   \n",
       "195       12.910         16.33           82.53      516.4          0.07941   \n",
       "545       13.620         23.23           87.19      573.2          0.09246   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "71         8.888         14.64           58.79      244.0          0.09783   \n",
       "106       11.640         18.33           75.17      412.5          0.11420   \n",
       "270       14.290         16.82           90.30      632.6          0.06429   \n",
       "435       13.980         19.62           91.12      599.5          0.10600   \n",
       "102       12.180         20.52           77.22      458.7          0.08013   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "149           0.06376         0.02881              0.01329         0.1473   \n",
       "124           0.07325         0.08092              0.02800         0.1422   \n",
       "421           0.18360         0.14500              0.06300         0.2086   \n",
       "195           0.05366         0.03873              0.02377         0.1829   \n",
       "545           0.06747         0.02974              0.02443         0.1664   \n",
       "..                ...             ...                  ...            ...   \n",
       "71            0.15310         0.08606              0.02872         0.1902   \n",
       "106           0.10170         0.07070              0.03485         0.1801   \n",
       "270           0.02675         0.00725              0.00625         0.1508   \n",
       "435           0.11330         0.11260              0.06463         0.1669   \n",
       "102           0.04038         0.02383              0.01770         0.1739   \n",
       "\n",
       "     mean fractal dimension  ...  worst radius  worst texture  \\\n",
       "149                 0.05580  ...        15.340          22.46   \n",
       "124                 0.05823  ...        14.260          22.75   \n",
       "421                 0.07406  ...        16.460          18.34   \n",
       "195                 0.05667  ...        13.880          22.00   \n",
       "545                 0.05801  ...        15.350          29.09   \n",
       "..                      ...  ...           ...            ...   \n",
       "71                  0.08980  ...         9.733          15.67   \n",
       "106                 0.06520  ...        13.140          29.26   \n",
       "270                 0.05376  ...        14.910          20.65   \n",
       "435                 0.06544  ...        17.040          30.80   \n",
       "102                 0.05677  ...        13.340          32.84   \n",
       "\n",
       "     worst perimeter  worst area  worst smoothness  worst compactness  \\\n",
       "149            97.19       725.9           0.09711            0.18240   \n",
       "124            91.99       632.1           0.10250            0.25310   \n",
       "421           114.10       809.2           0.13120            0.36350   \n",
       "195            90.81       600.6           0.10970            0.15060   \n",
       "545            97.58       729.8           0.12160            0.15170   \n",
       "..               ...         ...               ...                ...   \n",
       "71             62.56       284.4           0.12070            0.24360   \n",
       "106            85.51       521.7           0.16880            0.26600   \n",
       "270            94.44       684.6           0.08567            0.05036   \n",
       "435           113.90       869.3           0.16130            0.35680   \n",
       "102            84.58       547.8           0.11230            0.08862   \n",
       "\n",
       "     worst concavity  worst concave points  worst symmetry  \\\n",
       "149          0.15640               0.06019          0.2350   \n",
       "124          0.33080               0.08978          0.2048   \n",
       "421          0.32190               0.11080          0.2827   \n",
       "195          0.17640               0.08235          0.3024   \n",
       "545          0.10490               0.07174          0.2642   \n",
       "..               ...                   ...             ...   \n",
       "71           0.14340               0.04786          0.2254   \n",
       "106          0.28730               0.12180          0.2806   \n",
       "270          0.03866               0.03333          0.2458   \n",
       "435          0.40690               0.18270          0.3179   \n",
       "102          0.11450               0.07431          0.2694   \n",
       "\n",
       "     worst fractal dimension  \n",
       "149                  0.07014  \n",
       "124                  0.07628  \n",
       "421                  0.09208  \n",
       "195                  0.06949  \n",
       "545                  0.06953  \n",
       "..                       ...  \n",
       "71                   0.10840  \n",
       "106                  0.09097  \n",
       "270                  0.06120  \n",
       "435                  0.10550  \n",
       "102                  0.06878  \n",
       "\n",
       "[398 rows x 30 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "24fd39c2-9e40-4119-b79b-81b20f098b79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>421</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>398 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target\n",
       "149       1\n",
       "124       1\n",
       "421       1\n",
       "195       1\n",
       "545       1\n",
       "..      ...\n",
       "71        1\n",
       "106       1\n",
       "270       1\n",
       "435       0\n",
       "102       1\n",
       "\n",
       "[398 rows x 1 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "4e414477",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'C': 5}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "d60fec5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9618470265492964"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fcbfff78",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred=model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "c73ddfc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "       0, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0,\n",
       "       1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "df56b85f-1204-4044-8c39-e821f57b7b5f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>431</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>540</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>542</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>501</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>171 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Target\n",
       "204       1\n",
       "70        0\n",
       "131       0\n",
       "431       1\n",
       "540       1\n",
       "..      ...\n",
       "69        1\n",
       "542       1\n",
       "176       1\n",
       "501       0\n",
       "247       1\n",
       "\n",
       "[171 rows x 1 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "eb729e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "a7a3961e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,   4],\n",
       "       [  1, 107]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test,y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ba7db107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.94      0.96        63\n",
      "           1       0.96      0.99      0.98       108\n",
      "\n",
      "    accuracy                           0.97       171\n",
      "   macro avg       0.97      0.96      0.97       171\n",
      "weighted avg       0.97      0.97      0.97       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "2137adbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9707602339181286"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test,y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
